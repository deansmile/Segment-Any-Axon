{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the initial performance of the MedSAM off-the-shelf (code copied from example notebook) (https://colab.research.google.com/drive/19WNtRMbpsxeqimBlmJwtd1dzpaIvK2FZ?usp=sharing#scrollTo=MBWWGgitt9D2). \n",
    "\n",
    "Please download the model medsam_vit_b.pth to your directory ahead of running this notebook. https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN\n",
    "\n",
    "Clone both of the following repository as well. Some files from axondeepseg are changed slightly so that the import statements work outside the packages. Please reference the repo for these changes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5cdbc23b1932e42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import MedSAM.train_one_gpu\n",
    "!git clone https://github.com/axondeepseg/axondeepseg.git\n",
    "!git clone https://github.com/bowang-lab/MedSAM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a96597d3734eb7aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from skimage import io, transform\n",
    "import torch.nn.functional as F\n",
    "from axondeepseg.AxonDeepSeg.testing.segmentation_scoring import Metrics_calculator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T16:19:30.451306Z",
     "start_time": "2024-03-11T16:19:30.446453Z"
    }
   },
   "id": "aedbd94fdf987d0d",
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed,  # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg\n",
    "\n",
    "\n",
    "MedSAM_CKPT_PATH = \"medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T16:19:31.830096Z",
     "start_time": "2024-03-11T16:19:30.971463Z"
    }
   },
   "id": "58c25d3e3d395949",
   "execution_count": 188
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to also generate the bounding boxes from ground truth masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fa4ba312ac9ad00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_boxes(mask):\n",
    "    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n",
    "    for i in range(mask.shape[1]):\n",
    "        for j in range(mask.shape[0]):\n",
    "            if mask[i, j] != 0:\n",
    "                mask[i, j] = 1\n",
    "    print(mask)\n",
    "\n",
    "\n",
    "to_boxes(\"datasets/zenodo/labels/7_EM1_3_10D_N_P_0002.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54efca283df41e92",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def eval_image(img, ground_truth, plot=False):\n",
    "    img_np = cv2.imread(img)\n",
    "    if len(img_np.shape) == 2:\n",
    "        img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
    "    else:\n",
    "        img_3c = img_np\n",
    "    H, W, _ = img_3c.shape\n",
    "    img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
    "    img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "        img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    "    )  # normalize to [0, 1], (H, W, 3)\n",
    "    # convert the shape to (3, H, W)\n",
    "    img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    box_np = np.array([[0, 0, 200, 100]])  # edit: need to make it detect the entire image\n",
    "    # transfer box_np t0 1024x1024 scale \n",
    "    box_1024 = box_np / np.array([W, H, W, H]) * 1024\n",
    "    with torch.no_grad():\n",
    "        image_embedding = medsam_model.image_encoder(img_1024_tensor)  # (1, 256, 64, 64)\n",
    "\n",
    "    medsam_seg = medsam_inference(medsam_model, image_embedding, box_1024, H, W)\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(img_3c)\n",
    "        show_box(box_np[0], ax[0])\n",
    "        ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "        ax[1].imshow(img_3c)\n",
    "        show_mask(medsam_seg, ax[1])\n",
    "        show_box(box_np[0], ax[1])\n",
    "        ax[1].set_title(\"MedSAM Segmentation\")\n",
    "        plt.show()\n",
    "\n",
    "    # calculate dice score\n",
    "    ground_truth_mask_axon = cv2.imread(ground_truth, cv2.IMREAD_GRAYSCALE) // 255\n",
    "    m_axon = Metrics_calculator(medsam_seg, ground_truth_mask_axon)\n",
    "    return m_axon.pw_dice()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "437549de72975837",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot the performance of this image, for instance\n",
    "print(\n",
    "    f\"The dice score is {eval_image('datasets/zenodo/images/7_EM1_3_10D_N_P_0002.png', 'datasets/zenodo/labels/7_EM1_3_10D_N_P_0002.png', plot=True)}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8e3e1e312d41315",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the mean dice score across all the different datasets\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd54756b724e4b37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset):\n",
    "    images = Path(dataset).joinpath(\"images\").glob(\"*\")\n",
    "    num_images = 0\n",
    "    tot_dice = 0\n",
    "    for image in images:\n",
    "        label = Path(dataset).joinpath(\"labels\", image.name)\n",
    "        tot_dice += eval_image(str(image), str(label))\n",
    "        num_images += 1\n",
    "    print(f\"The mean dice score of {Path(dataset).name} is {tot_dice / num_images:.5f}\")\n",
    "    return tot_dice / num_images, num_images\n",
    "\n",
    "\n",
    "mean_dice_zenodo, num_zenodo = evaluate_dataset(\"datasets/zenodo\")\n",
    "mean_dice_osf, num_osf = evaluate_dataset(\"datasets/osfstorage-archive\")\n",
    "mean_dice_ads, num_ads = evaluate_dataset(\"datasets/data_axondeepseg_sem\")\n",
    "print(\n",
    "    f\"The mean dice score is {(mean_dice_zenodo * num_zenodo + mean_dice_osf * num_osf + mean_dice_ads * num_ads) / (num_zenodo + num_ads + num_osf):.5f}\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7665caadefc5571",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can start to finetune the model with these images. We will need to first augment the datasets and preprocess them to required format of MedSam's train_one_gpu script."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1659877ed349b5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from aug import *\n",
    "\n",
    "\n",
    "def dataset_to_npy(dataset):\n",
    "    images = Path(dataset).joinpath(\"images\").glob(\"*\")\n",
    "\n",
    "    def save_npy(image, image_np, label_myelin, label_axons, augmentation=None):\n",
    "        img_npy_path_m = Path(dataset).parent.joinpath(\"npy\", \"myelin\", \"imgs\")\n",
    "        gt_npy_path_m = Path(dataset).parent.joinpath(\"npy\", \"myelin\", \"gts\")\n",
    "        img_npy_path_a = Path(dataset).parent.joinpath(\"npy\", \"axon\", \"imgs\")\n",
    "        gt_npy_path_a = Path(dataset).parent.joinpath(\"npy\", \"axon\", \"gts\")\n",
    "        \n",
    "        if np.random.uniform() < 0.1: # 10% test dataset\n",
    "            img_npy_path_m = Path(dataset).parent.joinpath(\"test\", \"myelin\", \"imgs\")\n",
    "            gt_npy_path_m = Path(dataset).parent.joinpath(\"test\", \"myelin\", \"gts\")\n",
    "            img_npy_path_a = Path(dataset).parent.joinpath(\"test\", \"axon\", \"imgs\")\n",
    "            gt_npy_path_a = Path(dataset).parent.joinpath(\"test\", \"axon\", \"gts\")\n",
    "        \n",
    "        with open(str(img_npy_path_m.joinpath(\n",
    "            f\"{Path(dataset).name}-{image.name}{'-'+augmentation if augmentation is not None else ''}.npy\"\n",
    "        )), 'wb') as f:\n",
    "            np.save(f, image_np)\n",
    "        with open(str(img_npy_path_a.joinpath(\n",
    "            f\"{Path(dataset).name}-{image.name}{'-'+augmentation if augmentation is not None else ''}.npy\"\n",
    "        )), 'wb') as f:\n",
    "            np.save(f, image_np)\n",
    "        with open(str(gt_npy_path_m.joinpath(\n",
    "            f\"{Path(dataset).name}-{image.name}{'-'+augmentation if augmentation is not None else ''}.npy\"\n",
    "        )), 'wb') as f:\n",
    "            np.save(f, label_myelin)\n",
    "        with open(str(gt_npy_path_a.joinpath(\n",
    "            f\"{Path(dataset).name}-{image.name}{'-'+augmentation if augmentation is not None else ''}.npy\"\n",
    "        )), 'wb') as f:\n",
    "            np.save(f, label_axons)\n",
    "\n",
    "    augmentations = [\n",
    "        random_shift_2d,\n",
    "        random_rotation_2d,\n",
    "        random_rescale_2d,\n",
    "        random_flip_2d,\n",
    "        random_blur_2d\n",
    "    ]\n",
    "\n",
    "    for image in images:\n",
    "        label = Path(dataset).joinpath(\"labels\", image.name)\n",
    "        label = cv2.imread(str(label), cv2.IMREAD_GRAYSCALE)\n",
    "        label = transform.resize(label, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
    "        label_axons = np.zeros(label.shape)\n",
    "        label_myelin = np.zeros(label.shape)\n",
    "\n",
    "        image_np = cv2.imread(str(image), cv2.IMREAD_GRAYSCALE)\n",
    "        image_np = transform.resize(image_np, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(\n",
    "            np.uint8)\n",
    "        image_np = (image_np - image_np.min()) / np.clip(\n",
    "            image_np.max() - image_np.min(), a_min=1e-8, a_max=None\n",
    "        )\n",
    "        for i in range(label.shape[1]):\n",
    "            for j in range(label.shape[0]):\n",
    "                if label[i, j] == 255:\n",
    "                    label_axons[i, j] = 1\n",
    "                if label[i, j] == 127:\n",
    "                    label_myelin[i, j] = 1\n",
    "\n",
    "        save_npy(image, np.dstack([image_np for _ in range(3)]), label_myelin, label_axons)\n",
    "\n",
    "        for augmentation in augmentations:\n",
    "            augmented_image = augmentation(image_np)\n",
    "            augmented_axon = augmentation(label_axons)\n",
    "            augmented_myelin = augmentation(label_myelin)\n",
    "\n",
    "            save_npy(image, np.dstack([augmented_image for _ in range(3)]), augmented_axon, augmented_myelin, augmentation.__name__)\n",
    "\n",
    "dataset_to_npy(\"datasets/data_axondeepseg_sem\")\n",
    "dataset_to_npy(\"datasets/zenodo\")\n",
    "dataset_to_npy(\"datasets/osfstorage-archive\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T16:30:41.879389Z",
     "start_time": "2024-03-11T16:28:39.062135Z"
    }
   },
   "id": "4a65f2a1dfb9ef9",
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training. You need to either remove the sanity check from the training file or install the CT dataset\n",
    "for it to function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fe6677fd7019a59"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shing6444/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\r\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\r\n",
      "Number of total parameters:  93735472\r\n",
      "Number of trainable parameters:  93729252\r\n",
      "Number of image encoder and mask decoder parameters:  93729252\r\n",
      "number of images: 127\r\n",
      "Number of training samples:  127\r\n",
      "  0%|                                                    | 0/64 [00:00<?, ?it/s]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/shing6444/PycharmProjects/Segment-Any-Axon/MedSAM/train_one_gpu.py\", line 350, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/shing6444/PycharmProjects/Segment-Any-Axon/MedSAM/train_one_gpu.py\", line 291, in main\r\n",
      "    for step, (image, gt2D, boxes, _) in enumerate(tqdm(train_dataloader)):\r\n",
      "  File \"/home/shing6444/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\r\n",
      "    for obj in iterable:\r\n",
      "  File \"/home/shing6444/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\r\n",
      "    data = self._next_data()\r\n",
      "  File \"/home/shing6444/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\r\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n",
      "  File \"/home/shing6444/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\r\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n",
      "  File \"/home/shing6444/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\r\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n",
      "  File \"/home/shing6444/PycharmProjects/Segment-Any-Axon/MedSAM/train_one_gpu.py\", line 96, in __getitem__\r\n",
      "    gt == random.choice(label_ids.tolist())\r\n",
      "  File \"/usr/lib/python3.10/random.py\", line 378, in choice\r\n",
      "    return seq[self._randbelow(len(seq))]\r\n",
      "IndexError: list index out of range\r\n"
     ]
    }
   ],
   "source": [
    "!python3 MedSAM/train_one_gpu.py --tr_npy_path datasets/npy/axon -checkpoint medsam_vit_b.pth -num_epochs 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T16:24:11.846456Z",
     "start_time": "2024-03-11T16:24:06.588004Z"
    }
   },
   "id": "36739873ecc720c9",
   "execution_count": 189
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "784bbd112661c94b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
